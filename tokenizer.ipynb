{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4865f9e3-eeb4-4b33-9fa3-7c325876278c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pulsar/miniconda3/envs/search-engine/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7b1c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "dataset = load_dataset(\"ms_marco\", \"v1.1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7033711d-7e04-4424-a9a6-e8ba91e26e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(df):\n",
    "    queries = df['query'].tolist()\n",
    "    passage_container = df['passages'].tolist()\n",
    "    passage_texts = []\n",
    "    for item in passage_container:\n",
    "        for passage_text in item[\"passage_text\"]:\n",
    "            passage_texts.append(passage_text)\n",
    "\n",
    "    unique_passage_list = list(set(passage_texts))\n",
    "    return queries, unique_passage_list\n",
    "    \n",
    "def count_unique_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        words = content.split()\n",
    "        unique_words = set(words)\n",
    "        return len(unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "765f027e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_data = dataset[\"validation\"]\n",
    "val_df = pd.DataFrame(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "671e5ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[\"train\"]\n",
    "train_df = pd.DataFrame(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f87ba143",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = dataset[\"test\"]\n",
    "test_df = pd.DataFrame(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a70e7090",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_train, passagges_train = df_to_list(train_df)\n",
    "queries_val, passagges_val = df_to_list(val_df)\n",
    "queries_test, passagges_test = df_to_list(test_df)\n",
    "\n",
    "with open('corpus.txt', 'w') as file:\n",
    "    for query in queries_train:\n",
    "        file.write(f'{query}\\n')\n",
    "    for passage in passagges_train:\n",
    "        file.write(f'{passage}\\n')\n",
    "        \n",
    "    for query in queries_val:\n",
    "        file.write(f'{query}\\n')\n",
    "    for passage in passagges_val:\n",
    "        file.write(f'{passage}\\n')\n",
    "        \n",
    "    for query in queries_test:\n",
    "        file.write(f'{query}\\n')\n",
    "    for passage in passagges_test:\n",
    "        file.write(f'{passage}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f8befa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_word_count = count_unique_words('corpus.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8683987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "model_prefix = 'tokenizer'  # Prefix for the model files\n",
    "vocab_size = 16000  # Number of tokens in the vocabulary\n",
    "model_type = 'bpe'  # You can choose 'unigram' or 'bpe'\n",
    "\n",
    "# Train the SentencePiece model\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt',\n",
    "    model_prefix=model_prefix,\n",
    "    vocab_size=vocab_size,\n",
    "    model_type=model_type\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41fbaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1048501\n"
     ]
    }
   ],
   "source": [
    "print(unique_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36210559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained SentencePiece model\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(f'{model_prefix}.model')\n",
    "\n",
    "with open('corpus.txt', 'r') as file:\n",
    "    # Read the entire contents of the file into a string\n",
    "    file_contents = file.read()\n",
    "\n",
    "# Now, file_contents contains the text from the file as a string\n",
    "\n",
    "\n",
    "tokens = sp.encode_as_pieces(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "442e373a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197729583\n",
      "15705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' query_tokens = dict()\\npassage_tokens = dict()\\ntoken_id = 0\\nfor query in queries:\\n    one_query_tokens = sp.encode_as_pieces(query)\\n    for token in one_query_tokens:\\n        query_tokens[token] = token_id\\n        token_id +=1\\n    \\ntoken_id = 0\\nfor unique_text in unique_passage_list:\\n    one_passage_tokens = sp.encode_as_pieces(unique_text)\\n    for token in one_passage_tokens:\\n        passage_tokens[token] = token_id\\n        token_id +=1\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(tokens))\n",
    "token_id = 1\n",
    "token_dictionary_valid = dict()\n",
    "token_dict = {}\n",
    "\n",
    "for token in tokens:\n",
    "    #token_dictionary_valid[str(token_id)] = token\n",
    "    token_dict[token] = token_id\n",
    "    token_id +=1\n",
    "print(len(token_dict))\n",
    "\n",
    "# Tokenize a sentence\n",
    "\"\"\" query_tokens = dict()\n",
    "passage_tokens = dict()\n",
    "token_id = 0\n",
    "for query in queries:\n",
    "    one_query_tokens = sp.encode_as_pieces(query)\n",
    "    for token in one_query_tokens:\n",
    "        query_tokens[token] = token_id\n",
    "        token_id +=1\n",
    "    \n",
    "token_id = 0\n",
    "for unique_text in unique_passage_list:\n",
    "    one_passage_tokens = sp.encode_as_pieces(unique_text)\n",
    "    for token in one_passage_tokens:\n",
    "        passage_tokens[token] = token_id\n",
    "        token_id +=1\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "627e08e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(df):\n",
    "    queries = df['query'].tolist()\n",
    "    passage_container = df['passages'].tolist()\n",
    "    passage_texts = []\n",
    "    for item in passage_container:\n",
    "        for passage_text in item[\"passage_text\"]:\n",
    "            passage_texts.append(passage_text)\n",
    "\n",
    "    unique_passage_list = list(set(passage_texts))\n",
    "    return queries, unique_passage_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f12c183",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      3\u001b[0m passage_container \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpassages\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(queries))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "    queries = df['query'].tolist()\n",
    "\n",
    "    passage_container = df['passages'].tolist()\n",
    "    \n",
    "    print(len(queries))\n",
    "    print(len(passage_container))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9b027075",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def construct_unique_passage_list(df):\n",
    "    passage_texts = []\n",
    "    for item in passage_container:\n",
    "        for passage_text in item[\"passage_text\"]:\n",
    "            passage_texts.append(passage_text)\n",
    "\n",
    "    return list(set(passage_texts))\n",
    "\n",
    "def text_to_tokenid(text):\n",
    "    token_list = sp.encode_as_pieces(text)\n",
    "    tokenid_list = []\n",
    "    for token in token_list:\n",
    "        if token in token_dict:\n",
    "            tokenid_list.append(token_dict[token])\n",
    "    return tokenid_list\n",
    "\n",
    "def construct_tokenid_lists(text_list):\n",
    "    tokenids_list = []\n",
    "    for text in text_list:\n",
    "        tokenids_list.append(text_to_tokenid(text))\n",
    "    return tokenids_list\n",
    "\n",
    "def list_to_labelled_dataset(df):\n",
    "    queries = df['query'].tolist()\n",
    "\n",
    "    passage_container = df['passages'].tolist()\n",
    "    \n",
    "    passage_texts = []\n",
    "    for item in passage_container:\n",
    "        for passage_text in item[\"passage_text\"]:\n",
    "            passage_texts.append(passage_text)\n",
    "    unique_passage_list = list(set(passage_texts))\n",
    "  #  print(len(unique_passage_list))\n",
    "    \n",
    "    query_tokens = []\n",
    "    passage_tokens = []\n",
    "    labels = []\n",
    "    \n",
    "    counter = 0\n",
    "    for item in passage_container:\n",
    "        for passage_text in item[\"passage_text\"]:\n",
    "            \n",
    "            query_tokens.append(queries[counter])\n",
    "            \n",
    "            passage_tokens.append(passage_text)\n",
    "            \n",
    "            labels.append(1)\n",
    "        \n",
    "        for _ in item[\"passage_text\"]:\n",
    "            \n",
    "            query_tokens.append(queries[counter])\n",
    "            \n",
    "            random_integer = random.randint(0, len(unique_passage_list)-1)\n",
    "            #print(random_integer)\n",
    "            passage_tokens.append(unique_passage_list[random_integer])\n",
    "            \n",
    "            labels.append(-1)\n",
    "        \n",
    "        counter+= 1\n",
    "        \n",
    "    query_tokenid_list = construct_tokenid_lists(query_tokens)\n",
    "    passage_tokenid_list = construct_tokenid_lists(passage_tokens)    \n",
    "       \n",
    "    print(len(passage_tokens))\n",
    "    print(passage_tokens[0])   \n",
    "    print(len(query_tokens))   \n",
    "    print(query_tokens[0]) \n",
    "    print(len(labels))   \n",
    "    print(labels[0])  \n",
    "        \n",
    "    return unique_passage_list, query_tokenid_list, passage_tokenid_list, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "68675123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352386\n",
      "Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\n",
      "1352386\n",
      "what is rba\n",
      "1352386\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "unique_passage_list_train, query_tokenid_list_train, passage_tokenid_list_train, train_labels = list_to_labelled_dataset(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11f2cf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164720\n",
      "The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.\n",
      "164720\n",
      "walgreens store sales average\n",
      "164720\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "unique_passage_list_val, query_tokenid_list_val, passage_tokenid_list_val, val_labels  = list_to_labelled_dataset(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cc6849a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158352\n",
      "We have been feeding our back yard squirrels for the fall and winter and we noticed that a few of them have missing fur. One has a patch missing down his back and under both arms. Also another has some missing on his whole chest. They are all eating and seem to have a good appetite.\n",
      "158352\n",
      "does human hair stop squirrels\n",
      "158352\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "unique_passage_list_test, query_tokenid_list_test, passage_tokenid_list_test, test_labels  = list_to_labelled_dataset(test_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e07bc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_passages_tokenids_list = construct_tokenid_lists(unique_passage_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30b7a9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1352386\n",
      "1352386\n",
      "1352386\n"
     ]
    }
   ],
   "source": [
    "print(len(query_tokenid_list_train))\n",
    "print(len(passage_tokenid_list_train))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4185ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "def export_list_of_lists(filename, data):\n",
    "    with open(filename, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(data)\n",
    "        \n",
    "def export_to_csv(filename, data):\n",
    "    with open(filename, \"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(data)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55b69357",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_csv(\"train_labels.csv\", train_labels)        \n",
    "export_list_of_lists(\"train_query_token_ids.csv\", query_tokenid_list_train)\n",
    "export_list_of_lists(\"train_document_token_ids.csv\", passage_tokenid_list_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "64e3370d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_list_of_lists(\"val_query_token_ids.csv\", query_tokenid_list_val)\n",
    "export_list_of_lists(\"val_document_token_ids.csv\", passage_tokenid_list_val)\n",
    "export_to_csv(\"val_labels.csv\",  val_labels)\n",
    "export_list_of_lists(\"test_query_token_ids.csv\", query_tokenid_list_test)\n",
    "export_list_of_lists(\"test_document_token_ids.csv\" , passage_tokenid_list_test)\n",
    "export_to_csv(\"test_labels.csv\" ,  test_labels )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4973f644",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_list_of_lists(\"cached_document_token_ids.csv\", unique_passages_tokenids_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a569e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
